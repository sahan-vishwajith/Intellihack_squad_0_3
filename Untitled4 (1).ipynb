{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pymupdf markdown langchain nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsUlBvXy_obG",
        "outputId": "2f8a8aaa-89ca-4f63-8427-68c395ec328b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.3)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.11/dist-packages (3.7)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.40)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mxolPIh1OnDF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DcHw4sPB-GDk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF for PDFs\n",
        "import markdown\n",
        "import re\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhaPnb_EDcoO",
        "outputId": "593c18ed-5d58-463f-fa19-4a934d2c8b2d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
        "    return text\n",
        "\n",
        "def extract_text_from_md(md_path):\n",
        "    \"\"\"Extracts text from a Markdown file.\"\"\"\n",
        "    with open(md_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "    # Strip frontmatter if it exists\n",
        "    text = re.sub(r'^---.*?---\\n', '', text, flags=re.DOTALL)\n",
        "    return markdown.markdown(text)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Cleans and normalizes text.\"\"\"\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces/newlines\n",
        "    return text\n",
        "\n",
        "def chunk_text(text, chunk_size=1024, overlap=100):\n",
        "    \"\"\"Splits text into chunks for embedding.\"\"\"\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=overlap, separators=[\"\\n\", \" \"]\n",
        "    )\n",
        "    return splitter.split_text(text)"
      ],
      "metadata": {
        "id": "wyniKCgE_tjY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_documents(folder_path):\n",
        "    \"\"\"Processes only PDF and Markdown files in a folder.\"\"\"\n",
        "    all_chunks = []\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # Process only .pdf and .md files\n",
        "        if file_name.endswith(\".pdf\"):\n",
        "            text = extract_text_from_pdf(file_path)\n",
        "        elif file_name.endswith(\".md\"):\n",
        "            text = extract_text_from_md(file_path)\n",
        "        else:\n",
        "            continue  # Skip unnecessary files\n",
        "\n",
        "        text = preprocess_text(text)\n",
        "        chunks = chunk_text(text)\n",
        "        all_chunks.extend(chunks)\n",
        "\n",
        "    return all_chunks\n",
        "\n",
        "# Run the script again\n",
        "folder_path = \"/content/\"\n",
        "chunks = process_documents(folder_path)\n",
        "print(f\"Processed {len(chunks)} text chunks!\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GerbCJBEAisJ",
        "outputId": "a5983fc9-d34c-41ed-cd95-5edc12e4b742"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 136 text chunks!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Preprocessing Steps"
      ],
      "metadata": {
        "id": "8Je1QGH9CBYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLIQya-sHNVh",
        "outputId": "d14797b3-1913-4563-a287-cee77861c922"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WioNlo1FKPPU",
        "outputId": "dce02f81-54f1-4b12-e6c2-1e495bf4410d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'chunks' is a list of text chunks produced by your document processing pipeline\n",
        "# For example, chunks = process_documents(folder_path)\n",
        "\n",
        "# Initialize the embedding model\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Generate embeddings for all chunks (ensure you're getting a numpy array)\n",
        "chunk_embeddings = embedding_model.encode(chunks, convert_to_tensor=False)\n",
        "chunk_embeddings = np.array(chunk_embeddings)  # Convert to numpy array if needed\n",
        "\n",
        "# Now, determine the embedding dimension\n",
        "embedding_dim = chunk_embeddings.shape[1]\n",
        "print(f\"Embedding dimension: {embedding_dim}\")\n",
        "\n",
        "# Create a FAISS index using L2 distance\n",
        "import faiss\n",
        "index = faiss.IndexFlatL2(embedding_dim)\n",
        "index.add(chunk_embeddings)  # Add your chunk embeddings to the index\n",
        "\n",
        "print(f\"FAISS index built with {index.ntotal} vectors.\")\n"
      ],
      "metadata": {
        "id": "XvPj_M-uF5Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_chunks(query: str, top_k: int = 5) -> list:\n",
        "    \"\"\"\n",
        "    Retrieves the top_k most relevant text chunks for a given query.\n",
        "\n",
        "    Args:\n",
        "        query (str): The input question/query.\n",
        "        top_k (int): Number of top chunks to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        list: List of retrieved text chunks.\n",
        "    \"\"\"\n",
        "    query_embedding = embedding_model.encode(query, convert_to_tensor=False)\n",
        "    query_embedding = np.array(query_embedding).reshape(1, -1)\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "    retrieved_chunks = [chunks[i] for i in indices[0]]\n",
        "    return retrieved_chunks\n"
      ],
      "metadata": {
        "id": "tAXOAuHdI8bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
      ],
      "metadata": {
        "id": "0OdeB88AJJJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
        "\n"
      ],
      "metadata": {
        "id": "2-BDxRwPI9bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(query: str) -> str:\n",
        "    # Retrieve relevant context chunks\n",
        "    retrieved_context = retrieve_chunks(query, top_k=5)\n",
        "    # Combine retrieved chunks into a single context block\n",
        "    context = \"\\n\".join(retrieved_context)\n",
        "\n",
        "    prompt = (\n",
        "    \"You are Qwen, an expert assistant in AI research. Based on the context provided below, \"\n",
        "    \"please answer the following question in a clear, concise, and informative manner. \"\n",
        "    \"Make sure your answer is accurate and directly addresses the question.\\n\\n\"\n",
        "    f\"Question: {query}\\n\\n\"\n",
        "    f\"Context:\\n{context}\\n\\n\"\n",
        "    \"Answer:\"\n",
        ")\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Use max_new_tokens instead of max_length to avoid truncating the prompt\n",
        "    outputs = model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.7)\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "\n",
        "# --- Example Usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    query = \"What advancements have been made in transformer architectures?\"\n",
        "    answer = generate_answer(query)\n",
        "    print(\"Generated Answer:\\n\", answer)"
      ],
      "metadata": {
        "id": "0aOKo3rwLMrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "id": "5LWV2lbHEFZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import nltk\n",
        "\n",
        "# Ensure NLTK tokenizer resources are available\n",
        "nltk.download('punkt')\n",
        "\n",
        "def load_dataset(file_path: str):\n",
        "    \"\"\"Load QA pairs from a JSON file.\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def compute_metrics(dataset: list) -> dict:\n",
        "    \"\"\"\n",
        "    Compute basic metrics for a dataset of QA pairs.\n",
        "\n",
        "    Returns a dictionary with:\n",
        "      - Total number of QA pairs\n",
        "      - Average question length (in words)\n",
        "      - Average answer length (in words)\n",
        "    \"\"\"\n",
        "    total_pairs = len(dataset)\n",
        "    total_question_length = 0\n",
        "    total_answer_length = 0\n",
        "\n",
        "    for pair in dataset:\n",
        "        question = pair.get(\"question\", \"\")\n",
        "        answer = pair.get(\"answer\", \"\")\n",
        "        total_question_length += len(nltk.word_tokenize(question))\n",
        "        total_answer_length += len(nltk.word_tokenize(answer))\n",
        "\n",
        "    avg_question_length = total_question_length / total_pairs if total_pairs else 0\n",
        "    avg_answer_length = total_answer_length / total_pairs if total_pairs else 0\n",
        "\n",
        "    return {\n",
        "        \"total_pairs\": total_pairs,\n",
        "        \"avg_question_length\": avg_question_length,\n",
        "        \"avg_answer_length\": avg_answer_length\n",
        "    }\n",
        "\n",
        "# Load the datasets\n",
        "train_dataset = load_dataset(\"train_qa.json\")\n",
        "val_dataset = load_dataset(\"val_qa.json\")\n",
        "test_dataset = load_dataset(\"test_qa.json\")\n",
        "\n",
        "# Compute metrics for each split\n",
        "train_metrics = compute_metrics(train_dataset)\n",
        "val_metrics = compute_metrics(val_dataset)\n",
        "test_metrics = compute_metrics(test_dataset)\n",
        "\n",
        "print(\"Train Metrics:\", train_metrics)\n",
        "print(\"Validation Metrics:\", val_metrics)\n",
        "print(\"Test Metrics:\", test_metrics)\n"
      ],
      "metadata": {
        "id": "o6yyCIEBDsMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Ensure NLTK tokenizer resources are available\n",
        "nltk.download('punkt')\n",
        "\n",
        "# --- Utility Functions for Normalization and Metrics ---\n",
        "\n",
        "def normalize_answer(s: str) -> str:\n",
        "    \"\"\"\n",
        "    Lower text and remove punctuation, articles and extra whitespace.\n",
        "    \"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "    def remove_punc(text):\n",
        "        return ''.join(ch for ch in text if ch not in set(string.punctuation))\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "    return white_space_fix(remove_articles(remove_punc(s.lower())))\n",
        "\n",
        "def compute_em_f1(prediction: str, ground_truth: str) -> (int, float):\n",
        "    \"\"\"\n",
        "    Compute Exact Match (EM) and F1 score between a prediction and a ground truth answer.\n",
        "    \"\"\"\n",
        "    norm_pred = normalize_answer(prediction)\n",
        "    norm_gt = normalize_answer(ground_truth)\n",
        "    em = int(norm_pred == norm_gt)\n",
        "\n",
        "    pred_tokens = norm_pred.split()\n",
        "    gt_tokens = norm_gt.split()\n",
        "    common = set(pred_tokens) & set(gt_tokens)\n",
        "    num_same = sum(min(pred_tokens.count(token), gt_tokens.count(token)) for token in common)\n",
        "\n",
        "    if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
        "        f1 = int(pred_tokens == gt_tokens)\n",
        "    else:\n",
        "        precision = num_same / len(pred_tokens)\n",
        "        recall = num_same / len(gt_tokens)\n",
        "        f1 = 0 if precision + recall == 0 else (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    return em, f1\n",
        "\n",
        "# --- Load the Test Dataset ---\n",
        "\n",
        "def load_dataset(file_path: str) -> list:\n",
        "    \"\"\"Load QA pairs from a JSON file.\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "test_dataset = load_dataset(\"test_qa.json\")\n",
        "print(f\"Loaded {len(test_dataset)} QA pairs from test_qa.json.\")\n",
        "\n",
        "# --- Define the Inference Function for Evaluation ---\n",
        "\n",
        "# Assumes you've already loaded your model and tokenizer.\n",
        "# For example:\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def generate_answer_for_evaluation(qa_pair: dict) -> str:\n",
        "    \"\"\"\n",
        "    Construct a prompt using the QA pair's context and question,\n",
        "    generate an answer using the model, and return the generated answer.\n",
        "    \"\"\"\n",
        "    prompt = f\"Question: {qa_pair['question']}\\nContext:\\n{qa_pair['context']}\\nAnswer:\"\n",
        "    model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    # Generate answer without sampling (deterministic)\n",
        "    generated_ids = model.generate(**model_inputs, max_new_tokens=128, do_sample=False)\n",
        "    output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    # Remove the prompt from the output if it is included\n",
        "    if output.startswith(prompt):\n",
        "        output = output[len(prompt):].strip()\n",
        "    return output\n",
        "\n",
        "# --- Evaluate the Model on the Test Set ---\n",
        "\n",
        "total_em = 0\n",
        "total_f1 = 0\n",
        "num_examples = len(test_dataset)\n",
        "\n",
        "print(\"\\nEvaluating on Test Set:\")\n",
        "for qa in test_dataset:\n",
        "    gt_answer = qa.get(\"answer\", \"\")\n",
        "    pred_answer = generate_answer_for_evaluation(qa)\n",
        "    em, f1 = compute_em_f1(pred_answer, gt_answer)\n",
        "    total_em += em\n",
        "    total_f1 += f1\n",
        "    print(\"Question:\", qa[\"question\"])\n",
        "    print(\"Ground Truth Answer:\", gt_answer)\n",
        "    print(\"Model Prediction:\", pred_answer)\n",
        "    print(\"EM:\", em, \"F1:\", f1, \"\\n\")\n",
        "\n",
        "avg_em = total_em / num_examples if num_examples > 0 else 0\n",
        "avg_f1 = total_f1 / num_examples if num_examples > 0 else 0\n",
        "print(\"Average Exact Match (EM):\", avg_em)\n",
        "print(\"Average F1 Score:\", avg_f1)\n"
      ],
      "metadata": {
        "id": "oDGy4YHCEnMQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}