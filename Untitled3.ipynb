{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvkchV7f_xKZ",
        "outputId": "b75bd399-8b16-41e7-b782-69975b3e68c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.25.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg8Gpgk7AUUA",
        "outputId": "acb22965-6f3c-4251-81e2-55e95ab9a6f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import markdown\n",
        "import fitz  # PyMuPDF for PDFs\n",
        "import random\n",
        "import nltk\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, pipeline\n",
        "import json\n",
        "\n",
        "# Download NLTK tokenizer resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# --- Step 1: Data Extraction, Preprocessing & Chunking ---\n",
        "\n",
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "    \"\"\"Extract text from a PDF file.\"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
        "        doc.close()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting PDF '{pdf_path}': {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def extract_text_from_md(md_path: str) -> str:\n",
        "    \"\"\"Extract text from a Markdown file.\"\"\"\n",
        "    try:\n",
        "        with open(md_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "        text = re.sub(r'^---.*?---\\n', '', text, flags=re.DOTALL)\n",
        "        return markdown.markdown(text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting Markdown '{md_path}': {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def preprocess_text(text: str, lowercase: bool = True) -> str:\n",
        "    \"\"\"Normalize whitespace and optionally lowercase text.\"\"\"\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text.lower() if lowercase else text\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = 1024, overlap: int = 100) -> list:\n",
        "    \"\"\"Split text into chunks that preserve semantic boundaries.\"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=overlap,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "    )\n",
        "    return splitter.split_text(text)\n",
        "\n",
        "def process_documents(folder_path: str) -> list:\n",
        "    \"\"\"Process supported files in a folder into text chunks.\"\"\"\n",
        "    all_chunks = []\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        if file_name.endswith(\".pdf\"):\n",
        "            text = extract_text_from_pdf(file_path)\n",
        "        elif file_name.endswith(\".md\"):\n",
        "            text = extract_text_from_md(file_path)\n",
        "        else:\n",
        "            continue\n",
        "        text = preprocess_text(text)\n",
        "        chunks = chunk_text(text)\n",
        "        all_chunks.extend(chunks)\n",
        "    return all_chunks\n",
        "\n",
        "# Set your document folder path (use a raw string for Windows paths)\n",
        "folder_path = \"/content/\"\n",
        "document_chunks = process_documents(folder_path)\n",
        "print(f\"Extracted {len(document_chunks)} text chunks.\")\n",
        "\n",
        "# Optional: Apply simple augmentation to expand the dataset\n",
        "def simple_augmentation(chunk: str) -> str:\n",
        "    sentences = nltk.sent_tokenize(chunk)\n",
        "    augmented = []\n",
        "    for sent in sentences:\n",
        "        augmented.append(sent)\n",
        "        if len(sent) > 20 and random.random() < 0.3:\n",
        "            augmented.append(sent + \" (augmented)\")\n",
        "    return \" \".join(augmented)\n",
        "\n",
        "def augment_data(chunks: list) -> list:\n",
        "    augmented_chunks = []\n",
        "    for chunk in chunks:\n",
        "        augmented_chunks.append(chunk)\n",
        "        if random.random() < 0.5:\n",
        "            augmented_chunks.append(simple_augmentation(chunk))\n",
        "    return augmented_chunks\n",
        "\n",
        "augmented_chunks = augment_data(document_chunks)\n",
        "print(f\"Augmented dataset size: {len(augmented_chunks)} chunks.\")\n",
        "\n",
        "# --- Step 2: Synthetic QA Generation using RAGAS Approach ---\n",
        "\n",
        "# Load a T5 model for question generation\n",
        "qg_model_name = \"valhalla/t5-small-e2e-qg\"\n",
        "qg_tokenizer = T5Tokenizer.from_pretrained(qg_model_name)\n",
        "qg_model = T5ForConditionalGeneration.from_pretrained(qg_model_name)\n",
        "\n",
        "# Load an extractive QA pipeline to produce answers\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "\n",
        "def generate_question(context: str) -> str:\n",
        "    \"\"\"Generate a question from context using a T5 model.\"\"\"\n",
        "    input_text = \"generate question: \" + context\n",
        "    inputs = qg_tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    outputs = qg_model.generate(inputs, max_length=64)\n",
        "    return qg_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "def generate_qa_pair(context: str) -> dict:\n",
        "    \"\"\"Generate a QA pair from a given context.\"\"\"\n",
        "    if len(context.split()) < 20:\n",
        "        return None\n",
        "    try:\n",
        "        question = generate_question(context)\n",
        "        qa_result = qa_pipeline(question=question, context=context)\n",
        "        answer = qa_result.get(\"answer\", \"\").strip()\n",
        "        return {\"context\": context, \"question\": question, \"answer\": answer}\n",
        "    except Exception as e:\n",
        "        print(f\"Error in QA generation: {e}\")\n",
        "        return None\n",
        "\n",
        "# Generate QA pairs for each augmented chunk\n",
        "synthetic_qa_pairs = []\n",
        "for chunk in augmented_chunks:\n",
        "    qa_pair = generate_qa_pair(chunk)\n",
        "    if qa_pair:\n",
        "        synthetic_qa_pairs.append(qa_pair)\n",
        "\n",
        "print(f\"Generated {len(synthetic_qa_pairs)} synthetic QA pairs.\")\n",
        "\n",
        "# --- Step 3: Quality Evaluation using DeepEval ---\n",
        "# (This is a placeholder for integration with a quality evaluation system.)\n",
        "# You would typically pass your QA pairs through DeepEval to get quality scores and filter accordingly.\n",
        "def evaluate_quality(qa_pair: dict) -> float:\n",
        "    # Dummy function: replace with actual DeepEval API call\n",
        "    return random.uniform(0.0, 1.0)\n",
        "\n",
        "quality_threshold = 0.5\n",
        "qa_pairs_filtered = [pair for pair in synthetic_qa_pairs if evaluate_quality(pair) >= quality_threshold]\n",
        "print(f\"Filtered dataset size after quality evaluation: {len(qa_pairs_filtered)} QA pairs.\")\n",
        "\n",
        "# --- Step 4: Data Curation with Kiln-AI ---\n",
        "# (This step might involve exporting the filtered data for human review and further augmentation.)\n",
        "# For the purpose of this pipeline, we assume that the filtered dataset is our final curated dataset.\n",
        "curated_qa_pairs = qa_pairs_filtered\n",
        "\n",
        "# --- Step 5: Dataset Splitting ---\n",
        "def split_dataset(qa_pairs: list, test_size: float = 0.1, val_size: float = 0.1, random_state: int = 42):\n",
        "    train_val, test = train_test_split(qa_pairs, test_size=test_size, random_state=random_state)\n",
        "    val_relative = val_size / (1 - test_size)\n",
        "    train, val = train_test_split(train_val, test_size=val_relative, random_state=random_state)\n",
        "    return train, val, test\n",
        "\n",
        "train_data, val_data, test_data = split_dataset(curated_qa_pairs)\n",
        "print(f\"Training set: {len(train_data)} QA pairs\")\n",
        "print(f\"Validation set: {len(val_data)} QA pairs\")\n",
        "print(f\"Test set: {len(test_data)} QA pairs\")\n",
        "\n",
        "# --- Optional: Save the datasets ---\n",
        "with open(\"train_qa.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(train_data, f, indent=2, ensure_ascii=False)\n",
        "with open(\"val_qa.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(val_data, f, indent=2, ensure_ascii=False)\n",
        "with open(\"test_qa.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(test_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"Datasets saved as train_qa.json, val_qa.json, and test_qa.json.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMblIEiF6Atv",
        "outputId": "99aa3039-c688-4586-da9a-bdfd0baa1ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 136 text chunks.\n",
            "Augmented dataset size: 208 chunks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 208 synthetic QA pairs.\n",
            "Filtered dataset size after quality evaluation: 113 QA pairs.\n",
            "Training set: 89 QA pairs\n",
            "Validation set: 12 QA pairs\n",
            "Test set: 12 QA pairs\n",
            "Datasets saved as train_qa.json, val_qa.json, and test_qa.json.\n"
          ]
        }
      ]
    }
  ]
}